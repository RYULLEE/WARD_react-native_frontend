import pandas as pd
pd.set_option("display.max_row", 100)
pd.set_option("display.max_column", 100)
import numpy as np
import os
import time
from tqdm import tqdm

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error as mse
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold
from lightgbm import LGBMRegressor

train_data = pd.read_csv("tier_system/train.csv")
test_data = pd.read_csv("tier_system/test.csv")

train_label = train_data["target"]
train_data = train_data.drop(["target"], axis=1)

test_label = test_data["target"]
test_data = test_data.drop(["target"], axis=1)

models = []

folds = KFold(n_splits=10)
for train_idx, val_idx in folds.split(train_data):
    
    train_x = train_data.iloc[train_idx, :]
    train_y = train_label[train_idx]
    val_x = train_data.iloc[val_idx, :]
    val_y = train_label[val_idx]
    
    model = LGBMRegressor(objective= "regression", max_depth= 5, n_estimators= 2000, learning_rate= 0.01, num_leaves = 31, n_jobs=-1)
    model.fit(train_x, train_y, eval_set=[(val_x, val_y)], eval_metric=["rmse"], early_stopping_rounds=300, verbose=300)
    models.append(model)
    
result = []
for i in models:
    result.append(i.predict(test_data))

predict = np.mean(result, axis = 0)
predict = pd.Series(predict)
difference = (test_label - predict)**2
size = len(test_label)
rmse = (difference.sum()/size)*0.5
print (rmse)